{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.style as style\n",
    "from matplotlib import rcParams\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "plt.style.use('Solarize_Light2')\n",
    "style.use('Solarize_Light2')\n",
    "%matplotlib inline\n",
    "print(plt.style.available)\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "rcParams['figure.figsize'] = 6,5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(pd.read_csv('BATADAL_trainingset1.csv')) # No attacks\n",
    "df_attacks = pd.DataFrame(pd.read_csv('BATADAL_trainingset2.csv')) # With attacks\n",
    "df_nolabels = pd.DataFrame(pd.read_csv('BATADAL_test_dataset.csv')) # With attacks no labels\n",
    "pd.set_option('display.expand_frame_repr', False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add missing attack labels\n",
    "The ATT_FLAG labels in dataset 2 in is incomplete, here we add the missing labels to the dataset.\n",
    "See https://batadal.net/images/Attacks_TrainingDataset2.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_attacks = df_attacks.set_index(\"DATETIME\")\n",
    "df_attacks[\" ATT_FLAG\"][\"26/09/16 11\":\"27/09/16 10\"] = 1 # Attack #2\n",
    "df_attacks[\" ATT_FLAG\"][\"29/10/16 19\":\"02/11/16 16\"] = 1 # Attack #4\n",
    "df_attacks[\" ATT_FLAG\"][\"26/11/16 17\":\"29/11/16 04\"] = 1 # Attack #5\n",
    "df_attacks[\" ATT_FLAG\"][\"06/12/16 07\":\"10/12/16 04\"] = 1 # Attack #6\n",
    "df_attacks[\" ATT_FLAG\"][\"14/12/16 15\":\"19/12/16 04\"] = 1 # Attack #7\n",
    "df_attacks = df_attacks.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1 - Familiarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_preproc = pd.DataFrame({\n",
    "    'date': df[\"DATETIME\"],\n",
    "    'F_PU1': df[\"F_PU1\"],\n",
    "    'F_PU2': df[\"F_PU2\"],\n",
    "    'F_PU4': df[\"F_PU4\"],\n",
    "    'F_PU7': df[\"F_PU7\"],\n",
    "})[1000:1500]\n",
    "data_preproc2 = pd.DataFrame({\n",
    "    'date': df[\"DATETIME\"],\n",
    "    'L_T1': df[\"L_T1\"],\n",
    "    'L_T3': df[\"L_T3\"],\n",
    "    'L_T5': df[\"L_T5\"],\n",
    "})[1000:1500]\n",
    "data_preproc3 = pd.DataFrame({\n",
    "    'date': df[\"DATETIME\"],\n",
    "    'P_J280': df[\"P_J280\"],\n",
    "    'P_J256': df[\"P_J256\"],\n",
    "    'P_J302': df[\"P_J302\"],\n",
    "    'P_J14': df[\"P_J14\"],\n",
    "})[1000:1500]\n",
    "\n",
    "data_preproc.plot(figsize=(20,10), x='date')\n",
    "data_preproc2.plot(figsize=(20,10), x='date')\n",
    "data_preproc3.plot(figsize=(20,10), x='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "sns.heatmap(df.corr())\n",
    "values = df['F_PU1']\n",
    "plt.show()\n",
    "\n",
    "# Remove all columns with a perfect correlation: \n",
    "perfect_cor = ['S_PU1', 'F_PU3', 'S_PU3', 'F_PU5', 'S_PU5', 'F_PU9', 'S_PU9', 'ATT_FLAG']\n",
    "# check all the removed columns on their data (they all contain exactly the same value everywhere so they can be removed)\n",
    "final_columns = list(df.columns)\n",
    "for col in perfect_cor:\n",
    "    print(df[col].value_counts())\n",
    "    final_columns.remove(col)\n",
    "\n",
    "def trimm_correlated(df_in, threshold):\n",
    "    df_corr = df_in.corr(method='pearson', min_periods=1)\n",
    "    df_not_correlated = ~(df_corr.mask(np.tril(np.ones([len(df_corr)]*2, dtype=bool))).abs() > threshold).any()\n",
    "    un_corr_idx = df_not_correlated.loc[df_not_correlated[df_not_correlated.index] == True].index\n",
    "    df_out = df_in[un_corr_idx]\n",
    "    return df_out\n",
    "\n",
    "new_df = trimm_correlated(df[final_columns], 0.8)\n",
    "sns.heatmap(new_df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def moving_average_prediction(data, window = 3):\n",
    "    test = [data[i] for i in range(window, len(data))]\n",
    "    predictions = []\n",
    "    \n",
    "    current_prediction = window\n",
    "    for t in range(len(test)):\n",
    "        predicted_value = mean([data[i] for i in range(current_prediction-window,current_prediction)])\n",
    "        predictions.append(predicted_value)\n",
    "        current_prediction += 1\n",
    "    # \tprint('predicted=%f, expected=%f' % (yhat, obs))\n",
    "    \n",
    "    error = mean_squared_error(test, predictions)\n",
    "    print('Test MSE: %.3f' % error)\n",
    "    return test, predictions\n",
    "\n",
    "\n",
    "print('F_PU1 window 1')\n",
    "data, predictions = moving_average_prediction(df['F_PU1'].values, 1)\n",
    "print('F_PU1 window 2')\n",
    "data, predictions = moving_average_prediction(df['F_PU1'].values, 2)\n",
    "print('F_PU1 window 3')\n",
    "data, predictions = moving_average_prediction(df['F_PU1'].values, 3)\n",
    "print('F_PU1 window 4')\n",
    "data, predictions = moving_average_prediction(df['F_PU1'].values, 4)\n",
    "\n",
    "print('P_J14 window 1')\n",
    "data, predictions = moving_average_prediction(df['P_J14'].values, 1)\n",
    "print('P_J14 window 2')\n",
    "data, predictions = moving_average_prediction(df['P_J14'].values, 2)\n",
    "print('P_J14 window 3')\n",
    "data, predictions = moving_average_prediction(df['P_J14'].values, 3)\n",
    "print('P_J14 window 4')\n",
    "data, predictions = moving_average_prediction(df['P_J14'].values, 4)\n",
    "\n",
    "print('L_T1 window 1')\n",
    "data, predictions = moving_average_prediction(df['L_T1'].values, 1)\n",
    "print('L_T1 window 2')\n",
    "data, predictions = moving_average_prediction(df['L_T1'].values, 2)\n",
    "print('L_T1 window 3')\n",
    "data, predictions = moving_average_prediction(df['L_T1'].values, 3)\n",
    "print('L_T1 window 4')\n",
    "data, predictions = moving_average_prediction(df['L_T1'].values, 4)\n",
    "\n",
    "# plots\n",
    "pd.DataFrame({\"prediction\":predictions[1000:2000],\n",
    "            \"actual\": data[1000:2000]}).plot(figsize=(20,10))\n",
    "# zoom plot\n",
    "pd.DataFrame({\"prediction\":predictions[:100],\n",
    "            \"actual\": data[:100]}).plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 - ARMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scipy\n",
    "\n",
    "# Make sure you have statsmodels >0.9.0 as it fails to import statsmodels.api\n",
    "# see https://github.com/statsmodels/statsmodels/issues/5759\n",
    "%pip install git+https://github.com/statsmodels/statsmodels\n",
    "    \n",
    "# If the cell below this runs successfully you do NOT need this, especially the line 'import statsmodels.api as sm'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.api import qqplot\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 150\n",
    "rcParams['figure.figsize'] = 6,5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation function\n",
    "We calculate the autocorrelation and partial autocorrelation functions to make an informed descision about what ARMA parameters to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "ax1 = fig.add_subplot(211)\n",
    "# from statsmodels.graphics.tsaplots import plot_acf\n",
    "fig = sm.graphics.tsa.plot_acf(df['F_PU1'].values.squeeze(), lags=40, ax=ax1)\n",
    "ax2 = fig.add_subplot(212)\n",
    "fig = sm.graphics.tsa.plot_pacf(df['F_PU1'], lags=40, ax=ax2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"The higher the AR order gets, the lower the AIC gets.\" you care about the rate of change. When the AIC does not drop substantially with the increase of an AR term, the search can stop for that sensor. \n",
    "def test_arma_params(train_series, params):\n",
    "    # Find optimal parameters based on AIC \n",
    "    arma_mod = sm.tsa.ARMA(train_series, (0,0)).fit()\n",
    "    \n",
    "    zero_aic = arma_mod.aic\n",
    "    best_params = params[0]\n",
    "    lowest_aic = arma_mod.aic\n",
    "    prev_aic = arma_mod.aic\n",
    "    \n",
    "    print(f\"first aic is {prev_aic}\")\n",
    "    for param_set in params:\n",
    "        print(\"testing \" + str(param_set))\n",
    "        try:\n",
    "            arma_mod = sm.tsa.ARMA(train_series, param_set).fit()\n",
    "            print(str(arma_mod.aic))\n",
    "        except:\n",
    "            continue\n",
    "        print(f\"Change: {arma_mod.aic - prev_aic}, change vs first: {arma_mod.aic - zero_aic}\")\n",
    "        prev_aic = arma_mod.aic\n",
    "        if arma_mod.aic < lowest_aic:\n",
    "            lowest_aic = arma_mod.aic\n",
    "            best_params = param_set\n",
    "            \n",
    "    print('best params: ' + str(best_params))\n",
    "\n",
    "\n",
    "def do_arma(train_series, test_series, params, attack_flags):\n",
    "    print(f'####################################\\nCurrent Series: {train_series.name}\\n####################################')\n",
    "    train_model = sm.tsa.ARMA(train_series, params).fit()#method='mle', trend='nc')\n",
    "    test_model = sm.tsa.ARMA(test_series, params).fit(start_params = train_model.params)#, transpars = False, method='mle', trend='nc')\n",
    "\n",
    "    #The equations are somewhat simpler if the time series is first reduced to zero-mean by subtracting the sample mean. Therefore, we will work with the mean-adjusted series\n",
    "\n",
    "    # Plotting the residuals\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    resid = test_model.resid\n",
    "    ax = resid.plot(ax=ax);\n",
    "\n",
    "    # +\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax = fig.add_subplot(111)\n",
    "    fig = qqplot(resid, line='q', ax=ax, fit=True)\n",
    "    # -\n",
    "\n",
    "    # ## ARMA Model Autocorrelation\n",
    "    print(\"Autocorrelation plots:\")\n",
    "    fig = plt.figure(figsize=(12,8))\n",
    "    ax1 = fig.add_subplot(211)\n",
    "    fig = sm.graphics.tsa.plot_acf(resid.values.squeeze(), lags=40, ax=ax1)\n",
    "    ax2 = fig.add_subplot(212)\n",
    "    fig = sm.graphics.tsa.plot_pacf(resid, lags=40, ax=ax2)\n",
    "\n",
    "    # ## Prediction\n",
    "#     prediction = test_model.predict()\n",
    "#     pd.DataFrame({\"prediction\":prediction[100:400],\n",
    "#                 \"actual\": train_series[100:400]}).plot(figsize=(20,10))\n",
    "\n",
    "    # ## Anomaly detection    \n",
    "    resid = test_model.resid\n",
    "    std = np.std(resid)\n",
    "    anomaly_thresh = 2 * std\n",
    "    detected_anomalies = test_model.resid[(resid) > anomaly_thresh]\n",
    "    \n",
    "    test_model = pd.DataFrame({ 'ATT_FLAG': attack_flags })\n",
    "    tp=0\n",
    "    fp=0\n",
    "    for index, _ in detected_anomalies.items():\n",
    "        if attack_flags[index]==1:\n",
    "            tp+=1\n",
    "        else:\n",
    "            fp+=1\n",
    "    tn=test_model.loc[attack_flags==-999].shape[0]-fp\n",
    "    fn=test_model.loc[attack_flags==1].shape[0]-tp\n",
    "    acc=100.0*(tp+tn)/(tp+tn+fp+fn)\n",
    "    if (tp+fp)!=0:\n",
    "        prec= 100.0 *tp / (tp + fp)\n",
    "    else:\n",
    "        prec=0\n",
    "    print(f\"TP: {tp}\")\n",
    "    print(f\"FP: {fp}\")\n",
    "    print(f\"Accuracy: {acc}\")\n",
    "    print(f\"Precision: {prec}\")\n",
    "    return detected_anomalies, resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_absolute_err(y, yhat):\n",
    "    return np.mean((np.abs(y.sub(yhat).mean()) / yhat)) # or percent error = * 100\n",
    "\n",
    "def mean_forecast_err(y, yhat):\n",
    "    return y.sub(yhat).mean()\n",
    "\n",
    "def plot_attacks(residuals, attacks, detected_anomalies, show_range = (0,5000)):\n",
    "    show_from = show_range[0]\n",
    "    show_to = show_range[1]\n",
    "    detected_attacks = []\n",
    "    for a in range(len(df_attacks)):\n",
    "            if a in detected_anomalies:\n",
    "                detected_attacks.append(0.7)\n",
    "            else:\n",
    "                detected_attacks.append(-999)\n",
    "\n",
    "    detected_attacks = pd.DataFrame(detected_attacks)\n",
    "    plt.figure()\n",
    "    residuals = residuals - np.mean(resid)\n",
    "    plt.plot(residuals[show_from:show_to], label=\"residuals\")\n",
    "    plt.plot(attacks[show_from:show_to], label=\"Attacks\")\n",
    "    plt.plot(detected_attacks[show_from:show_to], label=\"Detected Attacks\")\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([np.min(residuals)*2,max(np.max(residuals)*1.5, 2)])\n",
    "    plt.legend()\n",
    "    plt.savefig(\"savedplot.png\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# param_sets = [(1,0), (2,0), (3,0), (4,0), (5,0), (6,0), (7,0), (8,0)] # best 2,0\n",
    "param_sets = [(2,0), (2,1), (2,2), (2,3), (2,4), (2,5), (2,6), (2,7), (2,8)] # best 2,4\n",
    "# test_arma_params(df['F_PU7'], param_sets)\n",
    "\n",
    "anomalies, resid = do_arma(df['F_PU7'], df_attacks[' F_PU7'], (2,3), df_attacks[' ATT_FLAG'])\n",
    "# Zoom in on especially Attack#5 and 6, which attacks F_PU7\n",
    "plot_attacks(resid, df_attacks[' ATT_FLAG'], anomalies, (3400,3900))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_sets = [(1,0), (2,0), (3,0), (4,0), (5,0), (6,0), (7,0), (8,0)] # best 5,0\n",
    "# param_sets = [(5,0), (5,1), (5,2), (5,3), (5,4), (5,5), (5,6), (5,7), (5,8)] # best 5,2\n",
    "# test_arma_params(df['L_T4'], param_sets)\n",
    "\n",
    "anomalies, resid = do_arma(df['L_T4'], df_attacks[' L_T4'], (0,0), df_attacks[' ATT_FLAG'])\n",
    "# Zoom in on especially Attack#5 and 6, which attacks F_PU7, affecting L_T4\n",
    "plot_attacks(resid, df_attacks[' ATT_FLAG'], anomalies, (3000,4000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# param_sets = [(1,0), (2,0), (3,0), (4,0), (5,0), (6,0), (7,0), (8,0)] # best 4,0\n",
    "# param_sets = [(4,0), (4,1), (4,2), (4,3), (4, 4), (4,5), (4,6)] # best 4,2\n",
    "# test_arma_params(df['L_T1'], param_sets)\n",
    "\n",
    "anomalies, resid = do_arma(df['L_T1'], df_attacks[' L_T1'], (4,2), df_attacks[' ATT_FLAG'])\n",
    "# zoom in on attacks 3 and 4 specifically\n",
    "plot_attacks(resid, df_attacks[' ATT_FLAG'], anomalies, (1500,3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_sets = [(1,0), (2,0), (3,0), (4,0), (5,0), (6,0), (7,0), (8,0)] # best 3,0\n",
    "# param_sets = [(3,0), (3,1), (3,2), (3,3), (3, 4), (3,5), (3,6)] # best 3,0 or 3,4\n",
    "# test_arma_params(df['L_T7'], param_sets)\n",
    "\n",
    "anomalies, resid = do_arma(df['L_T7'], df_attacks[' L_T7'], (3,0), df_attacks[' ATT_FLAG'])\n",
    "plot_attacks(resid, df_attacks[' ATT_FLAG'], anomalies, (1500,3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# param_sets = [(1,0), (2,0), (3,0), (4,0), (5,0), (6,0), (7,0), (8,0)] # best 5,0\n",
    "# param_sets = [(5,0), (5,1), (5,2), (5,3), (5,4), (5,5), (5,6)] # best 5,6\n",
    "# test_arma_params(df['P_J300'], param_sets)\n",
    "\n",
    "anomalies, resid = do_arma(df['P_J300'], df_attacks[' P_J300'], (5,6), df_attacks[' ATT_FLAG'])\n",
    "plot_attacks(resid, df_attacks[' ATT_FLAG'], anomalies, (1500, 3000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_sets = [(1,0), (2,0), (3,0), (4,0), (5,0), (6,0), (7,0), (8,0)] # best 2,0\n",
    "# param_sets = [(2,0), (2,1), (2,2), (2,3), (2,4), (2,5), (2,6)] # best 2,2 or 2,5\n",
    "# test_arma_params(df['F_PU10'], param_sets)\n",
    "\n",
    "anomalies, resid = do_arma(df['F_PU10'], df_attacks[' F_PU10'], (2,5), df_attacks[' ATT_FLAG'])\n",
    "plot_attacks(resid, df_attacks[' ATT_FLAG'], anomalies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "df = df.drop('DATETIME', axis=1)\n",
    "\n",
    "def normalize(df):\n",
    "    df_normalized = df.copy()\n",
    "    df_normalized = df_normalized\n",
    "\n",
    "    normalize = TimeSeriesScalerMeanVariance(mu=0, std=1)\n",
    "    for col in df:\n",
    "        df_normalized[col] = normalize.fit_transform(df_normalized[col])[0]\n",
    "\n",
    "    return df_normalized\n",
    "\n",
    "df_normalized = normalize(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_end_of_cell_marker": 2
   },
   "outputs": [],
   "source": [
    "## Residuals \n",
    "pca = PCA()\n",
    "pca.fit(df_normalized)\n",
    "df_inverse_transformed = pca.inverse_transform(df_normalized)\n",
    "pca_residual = df_normalized - df_inverse_transformed\n",
    "pca_residual = np.square(pca_residual)\n",
    "pca_residual_combined = pca_residual.sum(axis=1) \n",
    "\n",
    "figure, ax = plt.subplots()\n",
    "plt.xlabel('Data points')\n",
    "plt.ylabel('Residual')\n",
    "plt.figure()\n",
    "ax.plot(pca_residual_combined)\n",
    "figure.savefig('pcaresidual.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Drop the abnormalities\n",
    "indices_to_drop = np.where(pca_residual_combined > 2000)\n",
    "print(indices_to_drop)\n",
    "index = indices_to_drop[0]\n",
    "print('before', df_normalized.shape)\n",
    "df_cleaned = df_normalized.copy()\n",
    "for index in indices_to_drop:\n",
    "    row = df.iloc[index]\n",
    "    df_cleaned = df_normalized.drop(row.index)\n",
    "print('after', df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Re-normalize\n",
    "df_cleaned_normalized = normalize(df_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "## Find importance of each principal component\n",
    "pca = PCA()\n",
    "pca.fit(df_cleaned_normalized)\n",
    "x_axis = np.arange(1, df_cleaned_normalized.shape[1]+1, 1)\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Variance Captured')\n",
    "plt.plot(x_axis, pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Cummulative Variance\n",
    "cummulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "x_axis = np.arange(1, df_cleaned_normalized.shape[1]+1, 1)\n",
    "plt.xlabel('Principal components')\n",
    "plt.ylabel('Cummulative variance captured')\n",
    "plt.plot(x_axis, cummulative_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual is now low\n",
    "pca = PCA()\n",
    "pca.fit(df_cleaned_normalized)\n",
    "df_inverse_transformed = pca.inverse_transform(df_cleaned_normalized)\n",
    "pca_residual = df_cleaned_normalized - df_inverse_transformed\n",
    "pca_residual = np.square(pca_residual)\n",
    "pca_residual_combined = pca_residual.sum(axis=1) \n",
    "\n",
    "figure, ax = plt.subplots()\n",
    "plt.xlabel('Data points')\n",
    "plt.ylabel('Residual')\n",
    "plt.figure()\n",
    "ax.plot(pca_residual_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the test dataset\n",
    "test_dataset = normalize(df_attacks.drop('DATETIME', axis=1).drop(' ATT_FLAG', axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform PCA analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find threshold \n",
    "pca = PCA(n_components=15)\n",
    "# pca.fit(df_cleaned_normalized)\n",
    "transformed = pca.fit_transform(df_cleaned_normalized)\n",
    "df_inverse_transformed = pca.inverse_transform(transformed)\n",
    "pca_residual = df_cleaned_normalized - df_inverse_transformed\n",
    "pca_residual = np.square(pca_residual)\n",
    "pca_residual_combined = pca_residual.sum(axis=1) \n",
    "threshold_max = np.max(pca_residual_combined)\n",
    "threshold_min = np.min(pca_residual_combined)\n",
    "\n",
    "# analyse test set\n",
    "pca = PCA(n_components=15)\n",
    "pca.fit(test_dataset)\n",
    "transformed = pca.fit_transform(test_dataset)\n",
    "reconstructed = pca.inverse_transform(transformed)\n",
    "\n",
    "residual_pca = test_dataset - reconstructed\n",
    "residual_pca = np.square(residual_pca)\n",
    "residual_pca = residual_pca.sum(axis=1) \n",
    "\n",
    "# Find attacks\n",
    "attack_indices = np.where((residual_pca > threshold_max*2))\n",
    "attack_indices2 = np.where((residual_pca < threshold_min*0.5))\n",
    "\n",
    "all_detected_attacks = np.append(attack_indices[0], attack_indices2[0])\n",
    "\n",
    "TP = 0\n",
    "FP = 0\n",
    "for index in all_detected_attacks:\n",
    "    if index in list(df_attacks.loc[df_attacks[' ATT_FLAG']==1].index):\n",
    "        TP +=1\n",
    "    else:\n",
    "        FP +=1 \n",
    "\n",
    "print(f'TP={TP}\\nFP={FP}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def plot_attacks(residuals, attacks, detected_anomalies):\n",
    "    show_from = 0\n",
    "    show_to = 5000\n",
    "    detected_attacks = []\n",
    "    for a in range(len(attacks)):\n",
    "            if a in detected_anomalies:\n",
    "                detected_attacks.append(0.5)\n",
    "            else:\n",
    "                detected_attacks.append(-99)\n",
    "\n",
    "    detected_attacks = pd.DataFrame(detected_attacks)\n",
    "    plt.figure(figsize=[10,5])\n",
    "    residuals = residuals - np.mean(residuals)\n",
    "#     plt.plot(residuals[show_from:show_to], label=\"residuals (normalized)\")\n",
    "    plt.plot(attacks[show_from:show_to], label=\"Actual attacks\")\n",
    "    plt.plot(detected_attacks[show_from:show_to], label=\"Detected Attacks\")\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0,2])\n",
    "    plt.legend()\n",
    "    plt.savefig('pca_plot.png')\n",
    "    plt.show()\n",
    "    \n",
    "plot_attacks(residual_pca, df_attacks[' ATT_FLAG'], all_detected_attacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from tslearn.generators import random_walks\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.piecewise import SymbolicAggregateApproximation\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "def n_grams(n, data):\n",
    "    gram = []\n",
    "    for gr in ngrams(data, n):\n",
    "        gram.append(''.join(gr))\n",
    "    return gram\n",
    "\n",
    "def perform_sax(dataset, gram_number, symbols, segments):\n",
    "    scaler = TimeSeriesScalerMeanVariance(mu=0., std=np.std(dataset))  # Rescale time series\n",
    "    dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "    # SAX transform\n",
    "    sax = SymbolicAggregateApproximation(n_segments=segments, alphabet_size_avg=symbols)\n",
    "    sax_dataset_inv = sax.inverse_transform(sax.fit_transform(dataset))\n",
    "    # print(pd.DataFrame(sax_dataset_inv[0])[0].value_counts())\n",
    "#     sax_dataset_inv = sax.fit_transform(dataset)\n",
    "#     print(len(sax_dataset_inv[0]))\n",
    "\n",
    "    # Convert result to strings\n",
    "    df_sax = pd.DataFrame(sax_dataset_inv[0])\n",
    "    sax_series = df_sax[0]\n",
    "    \n",
    "    # Convert sax from numeric to characters\n",
    "    sax_values = sax_series.unique()\n",
    "    alphabet = 'abcdefghijklmnopqrstuvw'\n",
    "    sax_dict = {x : alphabet[i]  for i, x in enumerate(sax_values)}\n",
    "    sax_list = [sax_dict[x] for x in sax_series]\n",
    "    \n",
    "    # Convert the list of characters to n_grams based on input parameter\n",
    "    tri = n_grams(gram_number, sax_list)\n",
    "#     print(Counter(tri))\n",
    "    return tri\n",
    "\n",
    "def detect_anomaly(train, test, attacks, gram_number, symbols, segments):\n",
    "    train_tri = perform_sax(train, gram_number, symbols, segments)\n",
    "    test_tri = perform_sax(test, gram_number, symbols, segments)\n",
    "#     print(train_tri)\n",
    "#     print(test_tri)\n",
    "    tp, fp, tn, fn = 0, 0, 0, 0\n",
    "    anomaly_list = []\n",
    "    for i, tri in enumerate(test_tri):\n",
    "        attack = attacks[i]\n",
    "        if tri in train_tri:\n",
    "            if attack == 1:\n",
    "                fn += 1\n",
    "            else:\n",
    "                tn += 1\n",
    "        else:\n",
    "            anomaly_list.append(i)\n",
    "            if attack == 1:\n",
    "                tp += 1\n",
    "            else:\n",
    "                fp += 1\n",
    "    # Print scores\n",
    "    if tp == 0 and fp == 0:\n",
    "        return 'None', -1, list(), 0, 0\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "        tag = f'experiment, symbols: {symbols}, segments: {segments}, gram_number: {gram_number}; fn: {fn}; tn: {tn}; fp: {fp}; tp: {tp}'\n",
    "        return tag, precision, anomaly_list, fp, tp\n",
    "            \n",
    "def run_experiments(train, test, attack_indices):\n",
    "    \"\"\"\n",
    "    Function that varies the parameters of the sax function\n",
    "    \"\"\"\n",
    "    max_prec = 0\n",
    "    max_tag = 'None'\n",
    "    max_positives = 0\n",
    "    for symbols in range(1, 20, 1):\n",
    "        for segments in range(1, 250, 20):\n",
    "            for gram_number in range(1, 6):\n",
    "                    tag, precision, anom_list, fp, tp = detect_anomaly(train, test, attack_indices, gram_number, symbols, segments)\n",
    "                    positives = tp + fp\n",
    "                    if precision >= max_prec and positives > 20:\n",
    "                        max_prec = precision\n",
    "                        max_tag = tag\n",
    "                        max_positives = positives\n",
    "    return max_tag, max_prec\n",
    "\n",
    "attack_indices = df_attacks[' ATT_FLAG']\n",
    "for col in new_df.columns:\n",
    "    print(col)\n",
    "    max_tag, max_prec = run_experiments(new_df[col], df_attacks[' ' + col], attack_indices)\n",
    "    print(f'Max prec: {max_prec}, tag: {max_tag}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attacks( attacks, detected_anomalies):\n",
    "    show_from = 0\n",
    "    show_to = 5000\n",
    "    detected_attacks = []\n",
    "    for a in range(len(attacks)):\n",
    "            if a in detected_anomalies:\n",
    "                detected_attacks.append(0.5)\n",
    "            else:\n",
    "                detected_attacks.append(-99)\n",
    "\n",
    "    detected_attacks = pd.DataFrame(detected_attacks)\n",
    "    plt.figure(figsize=[10,5])\n",
    "    plt.plot(attacks[show_from:show_to], label=\"Actual attacks\")\n",
    "    plt.plot(detected_attacks[show_from:show_to], label=\"Detected Attacks\")\n",
    "\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0,2])\n",
    "    plt.legend()\n",
    "    plt.savefig('pca_plot.png')\n",
    "    plt.show()\n",
    "\n",
    "attack_indices = df_attacks[' ATT_FLAG']\n",
    "max_tag, max_prec, anomaly_list, fp, tp = detect_anomaly(new_df['L_T1'], df_attacks[' ' + 'L_T1'], attack_indices, 1, 5, 81)\n",
    "print(max_tag)\n",
    "plot_attacks(df_attacks[' ATT_FLAG'], anomaly_list)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
